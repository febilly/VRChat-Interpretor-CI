````markdown
# 实时语音识别（Qwen-ASR-Realtime）客户端事件

> 本文档介绍在与 Qwen-ASR Realtime API 的 WebSocket 会话中，客户端向服务端发送的事件。

**用户指南：**模型介绍、功能特性和示例代码请参见[实时语音识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-real-time-speech-recognition)

---

## <b>session.update</b>

用于更新会话配置，建议在 WebSocket 连接建立后首先发送该事件。建议在 WebSocket 连接建立成功后，立即发送此事件作为交互的第一步。如果未发送，系统将使用默认配置。

服务端成功处理此事件后，会发送 [`session.updated`](https://help.aliyun.com/zh/model-studio/qwen-tts-realtime-server-events#424ef2e774q9p) 事件作为确认。

### 参数说明

| 参数 | 类型 | 说明 |
| --- | --- | --- |
| **type** | string | 必选。事件类型。固定为`session.update`。 |
| **event_id** | string | 必选。事件 ID。 |
| **session** | object | 必选。包含会话配置的对象。 |
| session.input_audio_format | string | 可选。音频格式。支持`pcm`和`opus`。<br>默认值：`pcm`。 |
| session.sample_rate | integer | 可选。音频采样率（Hz）。支持`16000`和`8000`。<br>默认值：`16000`。<br>设置为 `8000` 时，服务端会先升采样到 16000Hz 再进行识别，可能引入微小延迟。建议仅在源音频为 8000Hz（如电话线路）时使用。 |
| session.input_audio_transcription | object | 可选。语音识别相关配置。 |
| session.input_audio_transcription.language | string | 可选。音频源语言。<br>取值范围参见 [ISO 639-1 标准](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes)。 |
| session.input_audio_transcription.corpus.text | string | 可选。ASR 语料文本。提供与业务场景强相关的专有词汇（如产品名、人名），可以提升模型对这些词汇的识别准确度。 |
| session.turn_detection | object | 可选。VAD（Voice Activity Detection，语音活动检测）配置。<br>它是启用/关闭 VAD 模式的开关：若将它设为 null，则将关闭 [VAD 模式](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-interaction-process#9b49887720jcw)，启用 [Manual 模式](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-interaction-process#ee09a3493fsuc)；反之则相反。 |
| session.turn_detection.type | string | 若`turn_dection`存在， 必选。固定为 `server_vad`。 |
| session.turn_detection.threshold | float | 可选。VAD 检测阈值。<br>默认值：`0.2`。<br>取值范围：`[-1, 1]`。<br>较低的阈值会提高 VAD 的灵敏度，可能将背景噪音误判为语音。较高的阈值则降低灵敏度，有助于在嘈杂环境中减少误触发。 |
| session.turn_detection.silence_duration_ms | integer | 可选。VAD 断句检测阈值（ms）。静音持续时长超过该阈值将被认为是语句结束。<br>默认值：`800`。<br>取值范围：`[200, 6000]`。<br>较低的值（如 300ms）可使模型更快响应，但可能导致在自然停顿处发生不合理的断句。较高的值（如 1200ms）可更好地处理长句内的停顿，但会增加整体响应延迟。 |

### 示例

```json
{
    "event_id": "event_123",
    "type": "session.update",
    "session": {
        "input_audio_format": "pcm",
        "sample_rate": 16000,
        "input_audio_transcription": {
            "language": "zh",
            "corpus": {
              "text": "ASR语料，用以改进模型识别效果"
            }
        },
        "turn_detection": {
            "type": "server_vad",
            "threshold": 0.5,
            "silence_duration_ms": 800
        }
    }
}
````

-----

## \<b\>input\_audio\_buffer.append\</b\>

用于将音频数据块追加到服务端的输入缓冲区。这是流式发送音频的核心事件。

**不同场景下的区别：**

  * **VAD 模式**：音频缓冲区用于语音活动检测，服务端会自动决定何时提交音频进行识别。
  * **非 VAD 模式**：客户端可以控制每个事件中的音频数据量，单个 `input_audio_buffer.append` 事件中的 `audio` 字段内容最大为 15 MiB。建议流式发送较小的音频块以获得更快的响应。

**重要提示**：服务端不会对`input_audio_buffer.append`事件发送任何确认响应。

### 参数说明

| 参数 | 类型 | 说明 |
| --- | --- | --- |
| **type** | string | 必选。事件类型。固定为`input_audio_buffer.append`。 |
| **event\_id** | string | 必选。事件 ID。 |
| **audio** | string | 必选。Base64 编码的音频数据。 |

### 示例

```json
{
  "event_id": "event_2728",
  "type": "input_audio_buffer.append",
  "audio": "<audio> by base64"
}
```

-----

## \<b\>input\_audio\_buffer.commit\</b\>

非 VAD 模式下，用于手动触发识别。此事件通知服务端，客户端已发送完一段完整的语音，将当前缓冲区内的所有音频数据作为一个整体进行识别。

\*\*禁用场景：\*\*VAD 模式。

服务端成功处理后，会发送 [`input_audio_buffer.committed`](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-server-events#1108a3764an0e) 事件作为确认响应。

### 参数说明

| 参数 | 类型 | 说明 |
| --- | --- | --- |
| **type** | string | 必选。事件类型。固定为`input_audio_buffer.commit`。 |
| **event\_id** | string | 必选。事件 ID。 |

### 示例

```json
{
  "event_id": "event_789",
   "type": "input_audio_buffer.commit"
}
```

```
```