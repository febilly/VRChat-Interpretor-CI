````md
[首页](/) > [大模型服务平台百炼](/zh/model-studio/) > [API参考（模型）](/zh/model-studio/model-api-reference/) > [语音合成/识别/翻译](/zh/model-studio/speech-synthesis-and-speech-recognition/) > [实时语音识别（Qwen-ASR-Realtime）](/zh/model-studio/qwen-asr-realtime-api/) > 交互流程

# 实时语音识别（Qwen-ASR-Realtime）交互流程

**更新时间：** 2025-10-29

---

实时语音识别-通义千问服务通过 WebSocket 协议，接收实时音频流并实时转写。支持 [VAD 模式](#vad-模式默认)和 [Manual 模式](#manual-模式)交互流程。

**用户指南：** 模型介绍、功能特性和示例代码请参见[实时语音识别-通义千问](https://help.aliyun.com/zh/model-studio/qwen-real-time-speech-recognition)

## URL

编码时，将 `<model_name>` 替换为实际的[模型](https://help.aliyun.com/zh/model-studio/qwen-real-time-speech-recognition#ff8c59ef0busr)。

```http
wss://[dashscope.aliyuncs.com/api-ws/v1/realtime?model=](https://dashscope.aliyuncs.com/api-ws/v1/realtime?model=)<model_name>
````

## Headers

```http
"Authorization": "bearer <your_dashscope_api_key>"
```

## VAD 模式（默认）

服务端自动检测语音的起点和终点（断句）。开发者只需持续发送音频流，服务端会在检测到一句话结束时自动返回最终识别结果。此模式适用于实时对话、会议记录等场景。

**启用方式：** 配置客户端 `[session.update](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-client-events#af43722339yva)` 事件的 `session.turn_detection` 参数。

[image]

  * 服务端在检测到语音开始时发送 `[input_audio_buffer.speech_started](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-server-events#038d846199nbm)` 事件。
  * 客户端通过发送 `[input_audio_buffer.append](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-client-events#a42f8e9111n72)` 事件将音频追加到缓冲区。
  * 服务端在检测到语音结束时发送 `[input_audio_buffer.speech_stopped](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-server-events#3d73b074cak7k)` 事件。
  * 服务端通过发送 `[input_audio_buffer.committed](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-server-events#1108a3764an0e)` 事件来提交输入音频缓冲区。
  * 服务端发送 `[conversation.item.created](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-server-events#04dabbb9b6eto)` 事件，其中包含从音频缓冲区创建的用户消息项。
  * 服务端发送 `[conversation.item.input_audio_transcription.text](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-server-events#ba1b5cdd79fxu)` 事件，其中包含语音识别结果。

## Manual 模式

由客户端控制断句。客户端需要发送完一整句话的音频后，再发送一个 `[input_audio_buffer.commit](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-client-events#d6d5cd90f3q4c)` 事件来通知服务端。此模式适用于客户端能明确判断语句边界的场景，如聊天软件中的发送语音。

**启用方式：** 将客户端 `[session.update](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-client-events#af43722339yva)` 事件的 `session.turn_detection` 设为 null。

[image]

  * 客户端通过发送 `[input_audio_buffer.append](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-client-events#a42f8e9111n72)` 事件将音频追加到缓冲区。
  * 客户端通过发送 `[input_audio_buffer.commit](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-client-events#d6d5cd90f3q4c)` 事件来提交输入音频缓冲区。 该提交会在对话中创建一个新的用户消息项。
  * 服务端发送 `[input_audio_buffer.committed](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-server-events#1108a3764an0e)` 事件进行响应。
  * 服务端发送 `[conversation.item.input_audio_transcription.text](https://help.aliyun.com/zh/model-studio/qwen-asr-realtime-server-events#ba1b5cdd79fxu)` 事件，其中包含语音识别结果。

<!-- end list -->

```
```